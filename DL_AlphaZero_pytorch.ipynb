{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"gsbRHboyPssl"},"outputs":[],"source":["import random\n","import math\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1676931100931,"user":{"displayName":"ぼくリンボク","userId":"01843774972273594658"},"user_tz":-540},"id":"Vc0EmNW6ecYg","outputId":"3df0f7ed-d73c-46eb-c21b-ac6b3ac3a810"},"outputs":[{"name":"stdout","output_type":"stream","text":["The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n","cpu\n"]}],"source":["import time\n","from collections import deque\n","import copy\n","import os\n","\n","import torch\n","from torch.distributions import Normal\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","# tensorboard用\n","from torch.utils.tensorboard import SummaryWriter\n","%load_ext tensorboard\n","\n","# GPUが使える環境なら使う\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19369,"status":"ok","timestamp":1676931124272,"user":{"displayName":"ぼくリンボク","userId":"01843774972273594658"},"user_tz":-540},"id":"L_G9BZHT3zfN","outputId":"f558f1e8-9efd-44a8-9fe8-74a3eb419807"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# Google DriveをColabにマウント\n","from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n","\n","\n","work_dir = '/content/drive/MyDrive/masa/'\n","os.chdir(work_dir)\n","\n","os.makedirs('./Alpha_pytorch_model/', exist_ok=True)  # フォルダがない時は生成"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"H_RyaH5vPvY1"},"source":["## 1.ゲーム系"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"f2jXFIF_dp1h"},"source":["### 1.1 ゲームの作成"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"K_dnRv1VFB-c"},"source":["学習させたいゲームを選んで実行"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"sdKPzQvTE8mK"},"source":["#### 1.1.1 三目並べ"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wXct4HGPP4_a"},"outputs":[],"source":["class State:  # ゲーム状態\n","    # 初期化\n","    def __init__(self, pieces=None, enemy_pieces=None):\n","        # 石の配置\n","        self.pieces = pieces if pieces != None else [0] * 9\n","        self.enemy_pieces = enemy_pieces if enemy_pieces != None else [0] * 9\n","\n","    # 石の数の取得\n","    def piece_count(self, pieces):\n","        count = 0\n","        for i in pieces:\n","            if i == 1:\n","                count += 1\n","        return count\n","\n","    # 負けかどうか\n","    def is_lose(self):\n","        # 3並びかどうか\n","        def is_comp(x, y, dx, dy):\n","            for k in range(3):\n","                if y < 0 or 2 < y or x < 0 or 2 < x or \\\n","                        self.enemy_pieces[x+y*3] == 0:\n","                    return False\n","                x, y = x+dx, y+dy\n","            return True\n","\n","        # 負けかどうか\n","        if is_comp(0, 0, 1, 1) or is_comp(0, 2, 1, -1):\n","            return True\n","        for i in range(3):\n","            if is_comp(0, i, 1, 0) or is_comp(i, 0, 0, 1):\n","                return True\n","        return False\n","\n","    # 引き分けかどうか\n","    def is_draw(self):\n","        return self.piece_count(self.pieces) + self.piece_count(self.enemy_pieces) == 9\n","\n","    # ゲーム終了かどうか\n","    def is_done(self):\n","        return self.is_lose() or self.is_draw()\n","\n","    # 次の状態の取得\n","    def next(self, action):\n","        pieces = self.pieces.copy()\n","        pieces[action] = 1\n","        return State(self.enemy_pieces, pieces)\n","\n","    # 合法手のリストの取得\n","    def legal_actions(self):\n","        actions = []\n","        for i in range(9):\n","            if self.pieces[i] == 0 and self.enemy_pieces[i] == 0:\n","                actions.append(i)\n","        return actions\n","\n","    # 先手かどうか\n","    def is_first_player(self):\n","        return self.piece_count(self.pieces) == self.piece_count(self.enemy_pieces)\n","\n","    # 文字列表示\n","    def __str__(self):\n","        ox = ('o', 'x') if self.is_first_player() else ('x', 'o')\n","        str = ''\n","        for i in range(9):\n","            if self.pieces[i] == 1:\n","                str += ox[0]\n","            elif self.enemy_pieces[i] == 1:\n","                str += ox[1]\n","            else:\n","                str += '-'\n","            if i % 3 == 2:\n","                str += '\\n'\n","        return str"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"aSVj7MxqWDGy"},"source":["#### 1.1.2 はじめに指を11,11として、指を選択して足していくゲーム"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V0hyOlpzImCE"},"outputs":[],"source":["class State:\n","    def __init__(self, hand=None, enemy_hand=None, count=0, max_count=50):\n","        self.hand = hand if hand != None else [1, 1]  # 自分の手\n","        self.enemy_hand = enemy_hand if enemy_hand != None else [1, 1]  # 相手の手\n","        self.count = count if count != 0 else 0  # 手数\n","        self.max_count = max_count  # 引き分けまでの手数\n","\n","    # 負けかどうか\n","    def is_lose(self):\n","        # 自分の手の合計が0 かつ 相手の手の合計が0以外\n","        return sum(self.hand)==0 and sum(self.enemy_hand)!=0\n","\n","    # 引き分けかどうか\n","    def is_draw(self):\n","        # 手数が上限値 かつ 両者の手が共に0以外\n","        return self.count == self.max_count and sum(self.hand)*sum(self.enemy_hand) != 0\n","    \n","    # ゲーム終了かどうか\n","    def is_done(self):\n","        return self.is_lose() or self.is_draw()\n","\n","    # 合法手の攻撃リストの取得\n","    def legal_attacks(self):\n","        attacks = []\n","        for i in range(2):\n","            for j in range(2):\n","                # 0じゃない手から0じゃない手に攻撃は可能\n","                if self.hand[i] != 0 and self.enemy_hand[j] != 0:\n","                    attacks.append(2*i+j)  # 左から左は0、左から右は1、右から左は2、右から右は3\n","        return attacks\n","\n","    # 合法手の分身リストの取得\n","    def legal_splits(self):\n","        splits = []\n","        hand_10 = self.hand[0]*10+self.hand[1]  # 10進数表記\n","        if hand_10 == 2 or hand_10 == 20:\n","            splits.append(11)\n","        if hand_10 == 3 or hand_10 == 30:\n","            splits.append(12)\n","            splits.append(21)\n","        if hand_10 == 4 or hand_10 == 40:\n","            splits.append(13)\n","            splits.append(22)\n","            splits.append(31)\n","        return splits\n","\n","    # 合法手のリストの取得\n","    def legal_actions(self):\n","        return self.legal_attacks()+self.legal_splits()  # 足し算でできる\n","\n","    # 次の状態の取得\n","    def next(self, action):\n","        # 行動を実行するときに手数を1増やして, 先手後手を入れ替える\n","        state = State(self.hand.copy(), self.enemy_hand.copy(), self.count+1)\n","        if action > 10:\n","            state.hand = [action//10, action%10]\n","        else:\n","            state.enemy_hand[action%2] += state.hand[action//2]\n","            state.enemy_hand[action%2] %= 5\n","        w = state.hand\n","        state.hand = state.enemy_hand\n","        state.enemy_hand = w\n","        return state\n","    \n","    # コメント表示\n","    def comment(self, action):\n","        str = '{}手目:'.format(self.count)\n","        str += '先手のターン...' if self.is_first_player() else '後手のターン...'\n","        if action>10:\n","            str += '{}と{}に分身'.format(action//10, action%10)\n","        else:\n","            dic = ['左', '右']\n","            str += '{}から{}へ攻撃'.format(dic[action//2], dic[action%2])\n","        return str\n","\n","    # 先手かどうか\n","    def is_first_player(self):\n","        # 2で割った余りが0のとき先手, 0手目は先手\n","        return self.count%2 == 0\n","\n","    # 文字列表示\n","    def __str__(self):\n","        # 先手を下に表示\n","        # 1回目の出力では先手ではないけど, 手が入れ替わっているのであっている.\n","        up = self.enemy_hand if self.is_first_player() else self.hand\n","        down = self.hand if self.is_first_player() else self.enemy_hand\n","        return '{}\\n{}'.format(up, down)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"gwR9Lr1dc-Hz"},"source":["### 1.2 行動選択の3つの方法"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"MJmK6_e3EXRo"},"source":["#### 1.2.1 ランダム"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K4nXCvaVSxxQ"},"outputs":[],"source":["def random_action(state):  # ランダムで行動選択\n","    legal_actions = state.legal_actions()\n","    return legal_actions[random.randint(0, len(legal_actions)-1)]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Oag1oZdNEoKG"},"source":["#### 1.2.2 アルファベータ法"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n2zMRzbHS11F"},"outputs":[],"source":["# アルファベータ法で状態価値を計算\n","def alpha_beta(state, alpha, beta, depth, max_depth=20):\n","    # depthは深さを表す. なんて先まで読むかを指定できる.\n","    # print(depth)\n","    if state.is_lose():\n","        return -1\n","    if state.is_draw():\n","        return 0\n","    if depth == max_depth:\n","        return 0\n","\n","    for action in state.legal_actions():\n","        score = -alpha_beta(state.next(action), -beta, -alpha, depth+1, max_depth)\n","        if score > alpha:\n","            alpha = score\n","        if alpha >= beta:\n","            return alpha\n","    return alpha\n","\n","# アルファベータ法で行動選択\n","def alpha_beta_action(state, max_depth=20):\n","    \"\"\"\n","    max_depth: 何手先まで予測するか\n","    \"\"\"\n","    best_action = 0\n","    alpha = -float('inf')\n","    actions = []\n","    scores = []\n","    for action in state.legal_actions():\n","        score = -alpha_beta(state.next(action), -float('inf'), -alpha, 1, max_depth=max_depth)\n","        # 始めの深さは1\n","        if score > alpha:\n","            best_action = action\n","            alpha = score\n","\n","        actions.append(action)\n","        scores.append(score)  # score:1は勝ち確定, score:0は相手がミスしないと引き分け, score:-1は相手がミスしないと負け確定\n","    # print(actions, scores)\n","    if max(scores) == 0:  # 最大スコアが0のときは, 0のものの中でランダムに選択\n","        actions = np.array(actions)\n","        scores = np.array(scores)\n","        actions_0 = actions[scores==0]\n","        best_action = actions_0[random.randint(0, len(actions_0)-1)]\n","\n","    return best_action"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-tNOluPxEwhw"},"source":["#### 1.2.3 モンテカルロ木探索"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yV61y54wTLsw"},"outputs":[],"source":["# モンテカルロ木探索用のランダムプレイアウト\n","def playout(state):\n","    if state.is_lose():\n","        return -1\n","    if state.is_draw():\n","        return 0\n","    return -playout(state.next(random_action(state)))\n","\n","\n","# モンテカルロ木探索で行動選択\n","def mcts_action(state, expansion=10, evaluation=100):\n","    \"\"\"\n","    expansion: 展開するまでの評価の回数\n","    evaluation: プレイアウトの試行回数\n","    \"\"\"\n","    class Node:  # モンテカルロ木探索のノードの定義\n","        def __init__(self, state):\n","            self.state = state\n","            self.w = 0  # 累計価値\n","            self.n = 0  # 試行回数\n","            self.child_nodes = None\n","\n","        def evaluate(self):  # evaluate：評価\n","            if self.state.is_done():  # ノードがゲーム終了まで行った場合。プレイアウトの報酬と一緒。\n","                # 相手が置いて自分のターンになったとき判定するので、負けか引き分けかしかない。\n","                value = -1 if self.state.is_lose() else 0\n","\n","                self.w += value\n","                self.n += 1\n","                return value\n","\n","            if not self.child_nodes:  # 一番下の葉のときplayout\n","                value = playout(self.state)\n","\n","                self.w += value\n","                self.n += 1\n","\n","                if self.n == expansion:  # 試行回数が10回をこえたら子ノードの展開\n","                    self.expand()  # 下で定義した関数へ\n","                return value\n","\n","            else:  # まだ下に葉が存在するとき、UCB1で求めた一つ下のノードでもう一回この関数を実行\n","                value = -self.next_child_node().evaluate()  # 下で定義した関数へ。この前にあるマイナスがポイント！\n","\n","                self.w += value  # 下のノードの価値の探索が終わったら、その価値の逆を上のノードの価値に伝播させる。\n","                self.n += 1\n","                return value\n","\n","        def expand(self):  # 子ノードの作成、上で使われる\n","            legal_actions = self.state.legal_actions()\n","            self.child_nodes = []\n","            for action in legal_actions:\n","                self.child_nodes.append(Node(self.state.next(action)))\n","\n","        def next_child_node(self):  # 下のどのノードを探索するか、上で使われる\n","            for child_node in self.child_nodes:  # 試行回数が0のものは優先的に\n","                if child_node.n == 0:\n","                    return child_node\n","\n","            t = 0  # t：全ての行動の試行回数\n","            for c in self.child_nodes:\n","                t += c.n\n","            ucb1_value = []\n","            for child_node in self.child_nodes:\n","                # 一項目のマイナスがポイント！子ノードは敵の価値なのでマイナス必要\n","                ucb1_value.append(-child_node.w/child_node.n + (2*np.log(t)/child_node.n)**0.5)\n","\n","            # どの行動をUCB1で探索しているかのデバッグ用\n","            # legal_actions = self.state.legal_actions()\n","            # print(legal_actions[np.argmax(ucb1_value)], end=' -> ')\n","\n","            return self.child_nodes[np.argmax(ucb1_value)]\n","\n","    root_node = Node(state)  # 今の局面のノードを作成\n","    root_node.expand()  # 一つ下は必ず探索する\n","\n","    for _ in range(evaluation):  # 100回探索する\n","        root_node.evaluate()\n","        # print()\n","\n","    legal_actions = state.legal_actions()\n","    n_list = []\n","    for c in root_node.child_nodes:  # 外部でクラス内の数値を呼ぶとき\n","        n_list.append(c.n)\n","    \n","    # print('a_list:', legal_actions)  # どの行動が取れるか\n","    # print('n_list:', n_list)  # 上の行動を100回のうち何回おこなったか\n","    # print('action:', legal_actions[np.argmax(n_list)])  # 結果どの行動をとるか\n","    return legal_actions[np.argmax(n_list)]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"JHFrPNkhQSYP"},"source":["### 1.3 ゲームのお試し"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"3jyXxSt4JFIP"},"source":["#### 1.3.1 一回だけ対戦"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":424,"status":"ok","timestamp":1676931142580,"user":{"displayName":"ぼくリンボク","userId":"01843774972273594658"},"user_tz":-540},"id":"fDcbnSFvQJAd","outputId":"bfb9476b-632d-4713-a895-161c2dde29ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["---\n","---\n","o--\n","\n","\n","---\n","-x-\n","o--\n","\n","\n","---\n","-xo\n","o--\n","\n","\n","---\n","-xo\n","ox-\n","\n","\n","-o-\n","-xo\n","ox-\n","\n","\n","xo-\n","-xo\n","ox-\n","\n","\n","xoo\n","-xo\n","ox-\n","\n","\n","xoo\n","-xo\n","oxx\n","\n","\n","後手の勝ち\n"]}],"source":["state = State()\n","while True:\n","    if state.is_done():\n","        if state.is_lose() and state.is_first_player():\n","            print('後手の勝ち')\n","        elif state.is_lose():\n","            print('先手の勝ち')\n","        else:\n","            print('引き分け')\n","        break\n","\n","    if state.is_first_player():\n","        action = random_action(state)  # ランダム\n","        # action = alpha_beta_action(state, max_depth=2)  # アルファベータ法\n","        # action = mcts_action(state, expansion=10, evaluation=100)  # モンテカルロ木探索\n","    else:\n","        # action = random_action(state)  # ランダム\n","        # action = alpha_beta_action(state, max_depth=20)  # アルファベータ法\n","        action = mcts_action(state, expansion=10, evaluation=100)  # モンテカルロ木探索\n","\n","    state = state.next(action)\n","    print(state)\n","    print()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"W3KFV7FrI-dA"},"source":["#### 1.3.2 複数回対戦"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xx-ZSW3xIxmG"},"outputs":[],"source":["ALL_GAME = 100\n","\n","    points=[]\n","    for i in range(ALL_GAME):\n","\n","        state = State()\n","        while True:\n","            if state.is_done():\n","                break\n","            if state.is_first_player():  # 対戦ルールを指定\n","                # action = random_action(state)  # ランダム\n","                # action = alpha_beta_action(state, max_depth=10)  # アルファベータ法\n","                action = mcts_action(state, expansion=10, evaluation=100)  # モンテカルロ木探索\n","            else:\n","                action = alpha_beta_action(state, max_depth=5)  # アルファベータ法\n","\n","            state = state.next(action)\n","\n","\n","        if state.is_lose():\n","            point = [0, 1, 0] if state.is_first_player() else [1, 0, 0]\n","        else:\n","            point = [0, 0, 1]\n","        points.append(point)\n","\n","        print('\\r試合数 {}/{}'.format(i + 1, ALL_GAME), end='')\n","    print()\n","\n","    all_point = np.sum(points, axis=0)\n","\n","    print('先手{}勝{}敗{}分'.format(all_point[0], all_point[1], all_point[2]))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"N1VjoOCscuXZ"},"source":["## 2.モデル"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zcQtGcfUfbya"},"outputs":[],"source":["class Dual(nn.Module):\n","    def __init__(self, hidden_dim=128):\n","        super().__init__()\n","        self.input_dim = 3*3*2\n","        self.action_dim = 9\n","        self.fc1 = nn.Linear(self.input_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3a = nn.Linear(hidden_dim, self.action_dim)  # 方策用\n","        self.fc3b = nn.Linear(hidden_dim, 1)  # 価値用\n","\n","    def forward(self, input):\n","        hidden = F.relu(self.fc1(input))\n","        hidden = F.relu(self.fc2(hidden))\n","        p = F.softmax(self.fc3a(hidden), dim=-1)\n","        v = torch.tanh(self.fc3b(hidden))\n","        return p, v\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"iEo8gcrNgud0"},"source":["## 3.Agent"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lT8ru4-dpuVf"},"source":["### 3.1 行動決定"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k7nkP_t-UPx8"},"outputs":[],"source":["PV_EVALUATE_COUNT = 50  # 1推論あたりのシミュレーション回数（本家は1600）\n","\n","def predict(model, state):\n","    x = torch.as_tensor([state.pieces, state.enemy_pieces], dtype=torch.float, device=device).view(-1, 18)\n","    p, v = model(x)\n","\n","    p = p[0][list(state.legal_actions())].detach().numpy()\n","    v = v[0][0].detach().numpy()\n","\n","    return p, v\n","\n","\n","def nodes_to_scores(nodes):  # ノードのリストを試行回数のリストに変換\n","    scores = []\n","    for c in nodes:\n","        scores.append(c.n)\n","    return scores  # [10, 15, ...]みたな訪問回数\n","\n","\n","def pv_mcts_scores(model, state, temperature):\n","    class Node:  # モンテカルロ木探索のノードの定義\n","        def __init__(self, state, p):  # クラスの定義に方策も追加\n","            self.state = state  # 状態\n","            self.p = p  # 方策\n","            self.w = 0  # 累計価値\n","            self.n = 0  # 試行回数\n","            self.child_nodes = None\n","\n","        def evaluate(self):\n","            # ゲーム終了時\n","            if self.state.is_done():  # ノードがゲーム終了まで行った場合。プレイアウトの報酬と一緒。\n","                value = -1 if self.state.is_lose() else 0  # 勝敗結果で価値を取得\n","\n","                self.w += value\n","                self.n += 1\n","                return value\n","\n","            if not self.child_nodes:  # 一番下の葉のとき\n","                policies, value = predict(model, self.state)  # playoutで求めていた価値をモデルから計算\n","\n","                self.w += value\n","                self.n += 1\n","\n","                # predictした瞬間、即展開。predictで1度価値を求めたら以後ここには来ない。\n","                self.child_nodes = []\n","                for action, policy in zip(self.state.legal_actions(), policies):\n","                    self.child_nodes.append(Node(self.state.next(action), policy))  # 方策とセット\n","                return value\n","\n","            else:  # まだ下に葉が存在するとき\n","                value = -self.next_child_node().evaluate()  # アーク評価値で枝をつたり、その価値を伝播\n","\n","                self.w += value\n","                self.n += 1\n","                return value\n","\n","        def next_child_node(self):  # アーク評価値が最大の子ノードを取得\n","            C_PUCT = 1.0\n","            t = sum(nodes_to_scores(self.child_nodes))\n","            pucb_values = []\n","            for child_node in self.child_nodes:\n","                pucb_values.append((-child_node.w / child_node.n if child_node.n else 0.0) +\n","                                    C_PUCT * child_node.p * np.sqrt(t) / (1 + child_node.n))\n","\n","            return self.child_nodes[np.argmax(pucb_values)]\n","\n","    # ここからmain\n","    root_node = Node(state, 0)  # 今の局面のノードを作成\n","\n","    for _ in range(PV_EVALUATE_COUNT):  # ルートノードを100回探索\n","        root_node.evaluate()\n","\n","    scores = nodes_to_scores(root_node.child_nodes)  # 訪問回数のリスト\n","\n","    return boltzman(scores, temperature)  # 合計1に直した訪問回数のリスト、次に選ばれる行動の確率。\n","\n","\n","def pv_mcts_action(model, temperature=0):  # モンテカルロ木探索で行動選択\n","    def pv_mcts_action(state):  # 関数に同じ名前の関数が入っていることに注意\n","        scores = pv_mcts_scores(model, state, temperature)\n","        return np.random.choice(state.legal_actions(), p=scores)\n","    return pv_mcts_action\n","\n","\n","def boltzman(xs, temperature):  # ボルツマン分布\n","    if temperature == 0:  # 最大値のみ1\n","        action = np.argmax(xs)\n","        scores = np.zeros(len(xs))\n","        scores[action] = 1\n","        return scores\n","    else:  # ボルツマン分布でバラつき付加\n","        xs = [x ** (1 / temperature) for x in xs]\n","        return [x / sum(xs) for x in xs]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":451,"status":"ok","timestamp":1676931150937,"user":{"displayName":"ぼくリンボク","userId":"01843774972273594658"},"user_tz":-540},"id":"Tf_0Gn8m9EjL","outputId":"b7deca25-fdd3-4578-a0a9-dc222b03504b"},"outputs":[{"name":"stdout","output_type":"stream","text":["---\n","-o-\n","---\n","\n","x--\n","-o-\n","---\n","\n","xo-\n","-o-\n","---\n","\n","xo-\n","-o-\n","-x-\n","\n","xo-\n","-o-\n","ox-\n","\n","xox\n","-o-\n","ox-\n","\n","xox\n","oo-\n","ox-\n","\n","xox\n","oox\n","ox-\n","\n","xox\n","oox\n","oxo\n","\n"]}],"source":["# デバッグ用\n","state = State()\n","model = Dual()\n","next_action = pv_mcts_action(model, 0)\n","\n","# ゲーム終了までループ\n","while True:\n","    # ゲーム終了時\n","    if state.is_done():\n","        break\n","\n","    # 行動の取得\n","    action = next_action(state)\n","\n","    # 次の状態の取得\n","    state = state.next(action)\n","\n","    # 文字列表示\n","    print(state)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"yEeOzOgJglRR"},"source":["### 3.2 リプレイバッファ"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Q6mniHYvtkI"},"outputs":[],"source":["class ReplayBuffer:\n","    def __init__(self, memory_size):\n","        self.memory = deque([], maxlen = memory_size)\n","    \n","    def append(self, transition):\n","        self.memory.append(transition)\n","    \n","    def sample(self, batch_size):\n","        batch_indexes = np.random.randint(0, len(self.memory), size=batch_size)\n","        pieces = np.array([self.memory[index]['piece'] for index in batch_indexes])  # 自分と相手の盤面\n","        actions = np.array([self.memory[index]['action'] for index in batch_indexes])  # 行動確率\n","        values = np.array([self.memory[index]['value'] for index in batch_indexes])  # 勝敗\n","        return {'pieces': pieces, 'actions': actions, 'values': values}"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"F5UmdVhijSyM"},"source":["## 4.学習"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wW7wkGD2y3EO"},"outputs":[],"source":["n_epochs = 10\n","lr=0.01\n","memory_size = 50000\n","batch_size = 128\n","\n","SP_GAME_COUNT = 50  # セルフプレイを行うゲーム数（本家は25000）\n","SP_TEMPERATURE = 1.0  # 訓練時のボルツマン分布の温度\n","\n","DN_OUTPUT_SIZE = 9\n","\n","RN_EPOCHS = 200  # 学習回数\n","\n","EN_GAME_COUNT = 50  # 1評価あたりのゲーム数（本家は400）\n","EN_TEMPERATURE = 1.0  # テスト時のボルツマン分布の温度\n","\n","\n","model = Dual()\n","# 損失関数の設定\n","# criterion = nn.BCELoss()  # マルチクラス分類用\n","criterion = nn.CrossEntropyLoss()  # softmax関数と通したあとloglossを計算\n","\n","# 最適化手法の設定\n","# optimizer = optim.SGD(params=params_to_update, lr=0.001, momentum=0.9)\n","optimizer = optim.Adam(model.parameters(), lr=0.001)  # 学習率は固定\n","\n","replay_buffer = ReplayBuffer(memory_size)\n","\n","if not os.path.exists('./Alpha_pytorch_model/best.pth'):\n","    torch.save(model.cpu().state_dict(), './Alpha_pytorch_model/best.pth')\n","    model = model.to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QZO0YsML4yMR"},"outputs":[],"source":["log_dir = 'logs'\n","writer = SummaryWriter(log_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y-C19_j5n2bT"},"outputs":[],"source":["def play(first_action, second_action):\n","    state = State()\n","    while True:\n","        if state.is_done():\n","            if state.is_lose():\n","                return [0, 1, 0] if state.is_first_player() else [1, 0, 0]\n","            else:\n","                return [0, 0, 1]\n","\n","        if state.is_first_player():\n","            action = action0(state)\n","        else:\n","            action = action1(state)\n","        state = state.next(action)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28487,"status":"ok","timestamp":1676936275135,"user":{"displayName":"ぼくリンボク","userId":"01843774972273594658"},"user_tz":-540},"id":"1c0SLlqiKh3z","outputId":"436ce977-8c50-43ff-c6e2-e84912e2a3fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["先手:ランダム, 後手:過去最強 | 先手 49勝1敗0分\n","先手:過去最強, 後手:ランダム | 後手 1勝48敗1分\n"]}],"source":["# action0 = random_action\n","action0 = alpha_beta_action\n","\n","best_model = Dual()\n","best_model.load_state_dict(torch.load('./Alpha_pytorch_model/best.pth'))\n","action1 = pv_mcts_action(best_model, 0)  # 過去最強のモデル\n","\n","first_point = np.array([0, 0, 0])\n","for i in range(50):\n","    first_point += np.array(play(action0, action1))\n","print('後手:過去最強 | 先手 {}勝{}敗{}分'.format(first_point[0], first_point[1], first_point[2]))\n","\n","second_point = np.array([0, 0, 0])\n","for i in range(50):\n","    second_point += np.array(play(action1, action0))\n","print('先手:過去最強 | 後手 {}勝{}敗{}分'.format(second_point[1], second_point[0], second_point[2]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":453,"status":"ok","timestamp":1676935567700,"user":{"displayName":"ぼくリンボク","userId":"01843774972273594658"},"user_tz":-540},"id":"2ZVPEM2w60uI","outputId":"bce3f92a-79a1-4ce5-844e-8303376bdbc8"},"outputs":[{"name":"stdout","output_type":"stream","text":["736\n"]}],"source":["print(len(replay_buffer.memory))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":188414,"status":"ok","timestamp":1676935825349,"user":{"displayName":"ぼくリンボク","userId":"01843774972273594658"},"user_tz":-540},"id":"SuJiYFd2oq7x","outputId":"0d1d79a6-fe6d-4da1-e0ce-13fe056c4bc5"},"outputs":[{"name":"stdout","output_type":"stream","text":["SelfPlay 50/50\n","200 /200\n","エポック: 1  loss: 2.107130\n","先手:最新, 後手:過去最強 | 先手 42勝5敗3分\n","先手:過去最強, 後手:最新 | 後手 5勝42敗3分\n","SelfPlay 50/50\n","200 /200\n","エポック: 2  loss: 1.927551\n","先手:最新, 後手:過去最強 | 先手 31勝17敗2分\n","先手:過去最強, 後手:最新 | 後手 14勝34敗2分\n","SelfPlay 50/50\n","200 /200\n","エポック: 3  loss: 1.863579\n","先手:最新, 後手:過去最強 | 先手 16勝17敗17分\n","先手:過去最強, 後手:最新 | 後手 12勝16敗22分\n","SelfPlay 50/50\n","200 /200\n","エポック: 4  loss: 1.838816\n","先手:最新, 後手:過去最強 | 先手 10勝37敗3分\n","先手:過去最強, 後手:最新 | 後手 39勝9敗2分\n","保存完了！\n","SelfPlay 50/50\n","200 /200\n","エポック: 5  loss: 1.801143\n","先手:最新, 後手:過去最強 | 先手 23勝22敗5分\n","先手:過去最強, 後手:最新 | 後手 21勝28敗1分\n","SelfPlay 50/50\n","200 /200\n","エポック: 6  loss: 1.822034\n","先手:最新, 後手:過去最強 | 先手 41勝6敗3分\n","先手:過去最強, 後手:最新 | 後手 3勝46敗1分\n","SelfPlay 50/50\n","200 /200\n","エポック: 7  loss: 1.797927\n","先手:最新, 後手:過去最強 | 先手 39勝8敗3分\n","先手:過去最強, 後手:最新 | 後手 8勝42敗0分\n","SelfPlay 50/50\n","200 /200\n","エポック: 8  loss: 1.832163\n","先手:最新, 後手:過去最強 | 先手 42勝8敗0分\n","先手:過去最強, 後手:最新 | 後手 9勝41敗0分\n","保存完了！\n","SelfPlay 50/50\n","200 /200\n","エポック: 9  loss: 1.834982\n","先手:最新, 後手:過去最強 | 先手 23勝27敗0分\n","先手:過去最強, 後手:最新 | 後手 16勝32敗2分\n","SelfPlay 50/50\n","200 /200\n","エポック: 10  loss: 1.844105\n","先手:最新, 後手:過去最強 | 先手 29勝21敗0分\n","先手:過去最強, 後手:最新 | 後手 13勝36敗1分\n"]}],"source":["for epoch in range(n_epochs):\n","    # -------------------------------------------------------------------------------------\n","    # 最新モデル同士で500回対戦\n","    # -------------------------------------------------------------------------------------\n","    for game_count in range(SP_GAME_COUNT):\n","        history = []\n","\n","        # 1ゲームの実行\n","        state = State()\n","\n","        while True:\n","            if state.is_done():  # ゲーム終了時\n","                break\n","\n","            # アーク評価値による訪問確率とその行動をデータに保存\n","            scores = pv_mcts_scores(model, state, SP_TEMPERATURE)\n","\n","            policies = [0] * DN_OUTPUT_SIZE\n","            for action, policy in zip(state.legal_actions(), scores):\n","                policies[action] = policy\n","\n","            history.append([[state.pieces, state.enemy_pieces], policies, None])\n","\n","            # scoresに基づいて行動し、次の状態を取得\n","            action = np.random.choice(state.legal_actions(), p=scores)\n","            state = state.next(action)\n","        \n","        if state.is_lose():\n","            value = -1 if state.is_first_player() else 1\n","        else:\n","            value = 0\n","\n","        for i in range(len(history)):\n","            history[i][2] = value  # 価値はエピソードの初めまで、割引無しで伝搬させる。\n","            value = -value  # 配置が交互に入れ替わっているので価値も反転させる\n","\n","        # 時系列を無視してリプレイバッファに保存\n","        for i in range(len(history)):\n","            transition = {'piece': history[i][0], 'action': history[i][1], 'value': history[i][2]}\n","            replay_buffer.append(transition)\n","\n","\n","        print('\\rSelfPlay {}/{}'.format(game_count+1, SP_GAME_COUNT), end='')\n","    print('')\n","\n","    # -------------------------------------------------------------------------------------\n","    # パラメータ更新部\n","    # 価値：状態から勝ち負けを予測する。\n","    # 方策：価値を用いたアーク評価値による訪問確率と、方策が一致するように学習していく。\n","    # -------------------------------------------------------------------------------------\n","    losses = []\n","    for update_step in range(RN_EPOCHS):\n","        # バッファから読み込む\n","        batch = replay_buffer.sample(batch_size)\n","        xs, y_policies, y_values = batch['pieces'], batch['actions'], batch['values']\n","\n","        # データの前処理\n","        xs = torch.as_tensor(xs, dtype=torch.float, device=device).view(-1, 18)  # Fratten\n","        y_policies = torch.as_tensor(y_policies, dtype=torch.float, device=device)\n","        y_values = torch.as_tensor(y_values, dtype=torch.float, device=device).view(-1, 1)\n","\n","        # 学習\n","        p, v = model(xs)\n","\n","        loss_p = criterion(p.cpu(), y_policies).to(device)\n","        loss_v = criterion(v.cpu(), y_values).to(device)\n","        loss = loss_p + loss_v  # lossは足し算\n","        loss.backward()  # 誤差伝搬\n","        optimizer.step()  # Adamでパラメータ更新\n","        losses.append(loss.cpu().detach().numpy())\n","\n","        print('\\r%3d /%3d' % (update_step+1, RN_EPOCHS), end='')\n","    print('')\n","    print('エポック: %d  loss: %lf' %(epoch+1, np.average(losses)))\n","    writer.add_scalar('train loss', loss.item(), epoch+1)\n","\n","    # -------------------------------------------------------------------------------------\n","    # 新パラメータ評価部\n","    # 色んな方法がある。「対ランダムにどれだけの割合で勝てるか」「過去の自分に勝てるか」\n","    # しかし三すくみみたいになってしまう可能性\n","    # 何なら、温度もハイパーパラメータみたいにしたい。学習時も相手と自分で違うモデルを作成すべき？\n","    # -------------------------------------------------------------------------------------\n","\n","    # 実際に検証したいときの、モデルの読み込み\n","    action0 = pv_mcts_action(model, EN_TEMPERATURE)\n","\n","    best_model = Dual()\n","    best_model.load_state_dict(torch.load('./Alpha_pytorch_model/best.pth'))\n","    action1 = pv_mcts_action(best_model, EN_TEMPERATURE)  # 過去最強のモデル\n","\n","    first_point = np.array([0, 0, 0])\n","    for i in range(EN_GAME_COUNT):\n","        first_point += np.array(play(action0, action1))\n","    print('先手:最新, 後手:過去最強 | 先手 {}勝{}敗{}分'.format(first_point[0], first_point[1], first_point[2]))\n","\n","    second_point = np.array([0, 0, 0])\n","    for i in range(EN_GAME_COUNT):\n","        second_point += np.array(play(action1, action0))\n","    print('先手:過去最強, 後手:最新 | 後手 {}勝{}敗{}分'.format(second_point[1], second_point[0], second_point[2]))\n","\n","    if first_point[0]+second_point[1] > first_point[1]+second_point[0]:\n","        torch.save(model.cpu().state_dict(), './Alpha_pytorch_model/best.pth')\n","        model = model.to(device)\n","        print('保存完了！')\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPybUfaKADeZK3u0JFLWmgh","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
